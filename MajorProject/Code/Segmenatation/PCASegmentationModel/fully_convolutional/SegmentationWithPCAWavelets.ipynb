{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb327ee",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb92c3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985217e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n",
      "Found GPU at: \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../segmentation_helper')\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import data_loader as dl\n",
    "import model_broker as mb\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tf.keras.backend.set_floatx(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbca875",
   "metadata": {},
   "source": [
    "# Dataset Loading\n",
    "\n",
    "This cell instantiates a data loader class, which is used for handling the dataset. A preproccesed image and segmentation dataset is loaded with the specified configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2598a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"pets\"\n",
    "test_size=300\n",
    "loader = dl.DataLoader(IMAGE_SIZE=128,dataset=dataset)\n",
    "img_ds = loader.import_processed_img()\n",
    "seg_ds = loader.import_processed_seg()\n",
    "cardinality = int(img_ds.cardinality())\n",
    "\n",
    "img_test = img_ds.take(test_size)\n",
    "seg_test = seg_ds.take(test_size)\n",
    "img_train = img_ds.skip(test_size)\n",
    "seg_train = seg_ds.skip(test_size)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff840d8d",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "These are possible activation functions that could be used. I couldn't get them working properly but they can be used to regularise the input if implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e341139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaledtanh(x): \n",
    "    return tf.math.tanh(x*0.1)\n",
    "\n",
    "def scaledatanh(x):\n",
    "    return tf.math.atanh(x)*10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a06574",
   "metadata": {},
   "source": [
    "# Image PCWN Training\n",
    "This cell trains a PCWN for the image dataset. The model broker is instantiated and used to build, load and check the model with the specified configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb70530",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_percent 0.3493347047096101\n",
      "meanimg.dtype <dtype: 'float64'>\n",
      "self.mean.dtype <dtype: 'float64'>\n",
      "self.mean.dtype <dtype: 'float64'>\n",
      "Starting level 0\n",
      "Completing 64.0\n",
      "pca shape tf.Tensor([27 27], shape=(2,), dtype=int32)\n",
      "keep_channels 9 keep_max 12.0\n",
      "keep_channels 9\n",
      "ufilts.shape (1, 1, 1, 27, 9)\n",
      "end loop 64.0\n",
      "Starting level 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\spurl\\AppData\\Local\\Temp\\__autograph_generated_fileth7s7l4u.py:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  ag__.if_stmt((ag__.ld(self).data_format is 'channels_first'), if_body, else_body, get_state, set_state, ('pad',), 1)\n",
      "C:\\Users\\spurl\\AppData\\Local\\Temp\\__autograph_generated_fileth7s7l4u.py:42: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  ag__.if_stmt((ag__.ld(self).data_format is 'channels_last'), if_body_1, else_body_1, get_state_1, set_state_1, ('pad',), 1)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"img\"\n",
    "img_broker = mb.ModelBroker(trainset=img_train,\n",
    "                            testset=img_test,\n",
    "                            dirname=dataset+\"_\"+model_name,\n",
    "                            keep_percent=0.3,\n",
    "                            count=5,\n",
    "                            sample_size=100)\n",
    "imghead,imginvhead = img_broker.build_model()\n",
    "imghead,imginvhead = img_broker.load_model()                                      \n",
    "img_broker.check_build(imghead,imginvhead,img_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16736c77",
   "metadata": {},
   "source": [
    "# Segmentation PCWN Training\n",
    "This cell trains a PCWN for the segmentation dataset. The model broker is instantiated and used to build, load and check the model with the specified configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccf8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"seg\"\n",
    "seg_broker = mb.ModelBroker(trainset=seg_train,\n",
    "                            testset=seg_test,\n",
    "                            dirname=dataset+\"_\"+model_name,\n",
    "                            keep_percent=0.3,\n",
    "                            count=5,\n",
    "                            sample_size=100)\n",
    "                            #activity_regularizer = scaledtanh,\n",
    "                            #inverse_activity_regularizer=scaledatanh,\n",
    "                            #activation_before=True)\n",
    "seghead,seginvhead = seg_broker.build_model()                                        \n",
    "seghead,seginvhead = seg_broker.load_model()\n",
    "seg_broker.check_build(seghead,seginvhead,seg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b983f2c",
   "metadata": {},
   "source": [
    "# Linear Least Squares Training (Convolutional)\n",
    "\n",
    "The following cells are used to find a matrix A and vector b which are used to map the image decomposition onto a segmentation decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee513b6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shape = imghead(next(iter(img_train))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8580451",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_0 = shape[3]\n",
    "shape_1 = shape[1]*shape[2]\n",
    "shape_2 = shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e941e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct = next(iter(seg_train))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fae39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0.0\n",
    "\n",
    "\n",
    "xxt = np.zeros([shape_0,shape_0])\n",
    "yxt = np.zeros([shape_0,shape_0])\n",
    "x = np.ones([shape_1])\n",
    "x_m = np.zeros([shape_0])\n",
    "y = np.ones([shape_1]) \n",
    "y_m = np.zeros([shape_0])\n",
    "\n",
    "bar = tqdm.notebook.tqdm(total = int(seg_train.cardinality()))\n",
    "\n",
    "for item in iter(zip(img_train,seg_train)):\n",
    "    bar.update(1)\n",
    "    image = item[0][0]\n",
    "    segmentation = item[1][0]\n",
    "    \n",
    "    imgdecom = imghead(image)\n",
    "    segdecom = seghead(segmentation)\n",
    "    \n",
    "    mat = tf.reshape(imgdecom,[-1,imgdecom.shape[-1]])\n",
    "    segmat = tf.reshape(segdecom,[-1,imgdecom.shape[-1]])\n",
    "    \n",
    "    cov = tf.tensordot(mat,mat,[0,0])\n",
    "    xxt += cov\n",
    "    #del cov\n",
    "    \n",
    "    segcov = tf.tensordot(mat,segmat,[0,0])\n",
    "    yxt += segcov\n",
    "    #del segcov\n",
    "    \n",
    "    x_m += tf.linalg.matvec(mat,x,transpose_a=True)\n",
    "    y_m += tf.linalg.matvec(segmat,y,transpose_a=True)\n",
    "    \n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c3c20",
   "metadata": {},
   "source": [
    "## Calculating A and B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d43fb5",
   "metadata": {},
   "source": [
    "This section uses the values found in the training loop to calculate values for A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aeb210",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxt = xxt - tf.matmul([x_m],[x_m],transpose_a=True)/n\n",
    "yxt = yxt - tf.matmul([x_m],[y_m],transpose_a=True)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_xxt = tf.linalg.pinv(xxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729db57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.matmul(inverse_xxt,yxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc01a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (y_m - tf.linalg.matvec(A,x_m,transpose_a=True))/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a880c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_filter = tf.reshape(A,(1,1,shape_0,shape_0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafc1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_coef(y_true, y_pred,smooth=1):\n",
    "  intersection = tf.reduce_sum(y_true * y_pred, 0)\n",
    "  union = tf.reduce_sum(y_true,0)+tf.reduce_sum(y_pred,0)-intersection\n",
    "  iou = tf.reduce_mean((intersection+1) / (union+1), 0)\n",
    "  return float(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred,smooth=1):\n",
    "    y_true_f = tf.reshape(y_true,-1)\n",
    "    y_pred_f =tf.reshape(y_pred,-1)\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f,0)\n",
    "\n",
    "    return float((2. * intersection+smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f)+smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_intensity = 0.09\n",
    "skip = np.random.randint(0,70)\n",
    "image,seg_base = next(iter(zip(img_test.skip(skip),seg_test.skip(skip))))\n",
    "imgdecom = imghead(image[0])\n",
    "\n",
    "conv = tf.nn.conv2d(imgdecom, A_filter,1,\"VALID\")\n",
    "conv = tf.nn.bias_add(conv,b)\n",
    "seg = seginvhead(conv)\n",
    "\n",
    "y_true = tf.cast(tf.reduce_min(seg_base[0],2)==0,tf.float64)\n",
    "y_pred = tf.cast(tf.reduce_min(seg[0],2)<threshold_intensity,tf.float64)\n",
    "\n",
    "print(dice_coef(y_true,y_pred))\n",
    "print(iou_coef(y_true,y_pred))\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(np.hstack([image[0],seg_base[0],seg[0]]))\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(np.hstack([y_true,y_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb558c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\",\"test\"]:\n",
    "    if split == \"train\":\n",
    "        img_ds, seg_ds = (img_train,seg_train)\n",
    "    else:\n",
    "        img_ds, seg_ds = (img_test,seg_test)\n",
    "    n = int(seg_ds.cardinality())\n",
    "    dice_coeff_vals = []\n",
    "    for image,seg_base in iter(zip(img_ds, seg_ds)):\n",
    "        imgdecom = imghead(image[0])\n",
    "        conv = tf.nn.conv2d(imgdecom, A_filter,1,\"VALID\")\n",
    "        conv = tf.nn.bias_add(conv,b)\n",
    "        seg = seginvhead(conv)\n",
    "        y_true = tf.cast(tf.reduce_min(seg_base[0],2)==0,tf.float64)\n",
    "        y_pred = tf.cast(tf.reduce_min(seg[0],2)<threshold_intensity,tf.float64)\n",
    "        dice_coeff_vals.append(dice_coef(y_true,y_pred))\n",
    "    dice_coeff_mean = sum(dice_coeff_vals)/n\n",
    "    dice_coeff_std = (sum([((x - dice_coeff_mean) ** 2) for x in dice_coeff_vals]) / n)**0.5\n",
    "    np.save(f\"PCWN_CONN_{dataset}_{split}\",dice_coeff_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb24c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
